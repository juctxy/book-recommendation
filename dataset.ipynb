{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPhokSGFmKK6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11b623b7-4f11-4023-f7ec-fe6c2bfa376b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<h1 class=\"novel-title text2row\" itemprop=\"name\">\r\n",
            "                        Transmigrated into the Gacha Game I Abandoned for 10 Years\r\n",
            "                    </h1>\n",
            "Springdrop\n",
            "I transmigrated into the gacha game I abandoned for 10 years.But, the state of the heroes I raised is strange.\n",
            "Action, Adventure, Fantasy, Harem\n",
            "Alchemy, Animal Characteristics, Anti-Magic, Army Building, Beastkin, Boss-Subordinate Relationship, Calm Protagonist, Caring Protagonist, Demons, Dense Protagonist, Dragons, Economics, Fantasy World, Game Elements, Game Ranking System, Heroes, Kingdom Building, Knights, Love Interest Falls in Love First, Love Rivals, Loyal Subordinates, Male Protagonist, Management, Misunderstandings, Modern Knowledge, Monsters, Multiple POV, Obsessive Love, Past Plays a Big Role, Poor to Rich, Strong Love Interests, Transported into a Game World, Weak Protagonist, Yandere, Younger Love Interests, \n",
            "3.8\n",
            "379\n"
          ]
        }
      ],
      "source": [
        "# import requests\n",
        "# import csv\n",
        "# from bs4 import BeautifulSoup\n",
        "\n",
        "# url = 'https://www.webnovelworld.org/novel/transmigrated-into-the-gacha-game-i-abandoned-for-10-years'  # Replace with the actual URL\n",
        "# headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "\n",
        "# response = requests.get(url, headers=headers)\n",
        "# soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# # Extract title\n",
        "# title_text = soup.find(\"h1\", class_=\"novel-title\")\n",
        "# title = title_text.text.strip() if title_text else \"N/A\"\n",
        "\n",
        "\n",
        "# # Extract summary\n",
        "# summary_text = \"N/A\"\n",
        "# summary_container = soup.find('div', class_='summary')\n",
        "# if summary_container:\n",
        "#     content_div = summary_container.find('div', class_='content expand-wrapper')\n",
        "#     if content_div:\n",
        "#         summary_text = content_div.get_text(strip=True).replace(\"Show More\", \"\").strip()\n",
        "\n",
        "# # Handle any encoding issues in the summary text\n",
        "# summary_text = summary_text.encode('utf-8', 'replace').decode('utf-8')\n",
        "\n",
        "# # Extract categories\n",
        "# categories = []\n",
        "# categories_container = soup.find('div', class_='categories')\n",
        "# if categories_container:\n",
        "#     category_links = categories_container.find_all('a', class_='property-item')\n",
        "#     categories = [cat.get_text(strip=True) for cat in category_links]\n",
        "\n",
        "# # Extract tags\n",
        "# tags = []\n",
        "# tags_container = soup.find('div', class_='tags')\n",
        "# if tags_container:\n",
        "#     tags_div = tags_container.find('div', class_='expand-wrapper')\n",
        "#     if tags_div:\n",
        "#         tags = [tag.get_text(strip=True).replace(\"Show More\", \"\") for tag in tags_div.find_all('a')]\n",
        "\n",
        "# # Merge categories and tags\n",
        "# categories_text = \", \".join(categories) if categories else \"N/A\"\n",
        "# tags_text = \", \".join(tags) if tags else \"N/A\"\n",
        "\n",
        "# # Extract author name\n",
        "# author_text = \"N/A\"\n",
        "# author_container = soup.find('div', class_='author')\n",
        "# if author_container:\n",
        "#     author_span = author_container.find('span', itemprop='author')\n",
        "#     if author_span:\n",
        "#         author_text = author_span.get_text(strip=True)\n",
        "\n",
        "# # Extract rating\n",
        "# rating_text = \"N/A\"\n",
        "# rating_container = soup.find('div', class_='rating-star')\n",
        "# if rating_container:\n",
        "#     rating_strong = rating_container.find('strong')\n",
        "#     if rating_strong:\n",
        "#         rating_text = rating_strong.get_text(strip=True)\n",
        "\n",
        "# # Extract rank\n",
        "# rank_text = \"N/A\"\n",
        "# rank_container = soup.find('div', class_='rank')\n",
        "# if rank_container:\n",
        "#     rank_strong = rank_container.find('strong')\n",
        "#     if rank_strong:\n",
        "#         rank_text = rank_strong.get_text(strip=True).replace(\"RANK\", \"\").strip()\n",
        "\n",
        "# print(title_text)\n",
        "# print(author_text)\n",
        "# print(summary_text)\n",
        "# print(categories_text)\n",
        "# print(tags_text)\n",
        "# print(rating_text)\n",
        "# print(rank_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4LwfVHWRS0SW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Function to extract book details from the novel page\n",
        "def extract_book_details(book_url):\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(book_url, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extract summary\n",
        "    summary_text = \"N/A\"\n",
        "    summary_container = soup.find('div', class_='summary')\n",
        "    if summary_container:\n",
        "        content_div = summary_container.find('div', class_='content expand-wrapper')\n",
        "        if content_div:\n",
        "            summary_text = content_div.get_text(strip=True).replace(\"Show More\", \"\").strip()\n",
        "\n",
        "    # Extract categories\n",
        "    categories = []\n",
        "    categories_container = soup.find('div', class_='categories')\n",
        "    if categories_container:\n",
        "        category_links = categories_container.find_all('a', class_='property-item')\n",
        "        categories = [cat.get_text(strip=True) for cat in category_links]\n",
        "\n",
        "    # Extract tags\n",
        "    tags = []\n",
        "    tags_container = soup.find('div', class_='tags')\n",
        "    if tags_container:\n",
        "        tags_div = tags_container.find('div', class_='expand-wrapper')\n",
        "        if tags_div:\n",
        "            tags = [tag.get_text(strip=True).replace(\"Show More\", \"\") for tag in tags_div.find_all('a')]\n",
        "\n",
        "    categories_text = \", \".join(categories) if categories else \"N/A\"\n",
        "    tags_text = \", \".join(tags) if tags else \"N/A\"\n",
        "\n",
        "    # Extract author name\n",
        "    author_text = \"N/A\"\n",
        "    author_container = soup.find('div', class_='author')\n",
        "    if author_container:\n",
        "        author_span = author_container.find('span', itemprop='author')\n",
        "        if author_span:\n",
        "            author_text = author_span.get_text(strip=True)\n",
        "\n",
        "    # Extract rating\n",
        "    rating_text = \"N/A\"\n",
        "    rating_container = soup.find('div', class_='rating-star')\n",
        "    if rating_container:\n",
        "        rating_strong = rating_container.find('strong')\n",
        "        if rating_strong:\n",
        "            rating_text = rating_strong.get_text(strip=True)\n",
        "\n",
        "    # Extract rank\n",
        "    rank_text = \"N/A\"\n",
        "    rank_container = soup.find('div', class_='rank')\n",
        "    if rank_container:\n",
        "        rank_strong = rank_container.find('strong')\n",
        "        if rank_strong:\n",
        "            rank_text = rank_strong.get_text(strip=True).replace(\"RANK\", \"\").strip()\n",
        "\n",
        "    # Extract title\n",
        "    title_text = soup.find(\"h1\")\n",
        "    title = title_text.get_text(strip=True) if title_text else \"N/A\"\n",
        "\n",
        "    # Extract chapter count\n",
        "    chapter_count = \"N/A\"\n",
        "\n",
        "    # Search for span containing chapter information\n",
        "    chapter_container = soup.find('div', class_='header-stats')  # Try to find the container for chapter info\n",
        "    if chapter_container:\n",
        "        # Look for the specific span with the number of chapters\n",
        "        strong_tag = chapter_container.find('strong')\n",
        "        if strong_tag:\n",
        "            chapter_count = strong_tag.get_text(strip=True)\n",
        "\n",
        "    return {\n",
        "        \"title\": title,\n",
        "        \"author\": author_text,\n",
        "        \"summary\": summary_text,\n",
        "        \"categories\": categories_text,\n",
        "        \"tags\": tags_text,\n",
        "        \"rating\": rating_text,\n",
        "        \"rank\": rank_text,\n",
        "        \"chapters\": chapter_count\n",
        "    }\n",
        "\n",
        "# Function to extract all href links from the main page's novel list\n",
        "def extract_all_links(main_url):\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(main_url, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    novel_list_container = soup.find('ul', class_='novel-list col5 m-col4')\n",
        "    href_links = []\n",
        "    if novel_list_container:\n",
        "        novel_list = novel_list_container.find_all('a', title=True)\n",
        "        for a in novel_list:\n",
        "            if 'href' in a.attrs:\n",
        "                href_links.append(a['href'])\n",
        "\n",
        "    return list(set(href_links))\n",
        "\n",
        "# Function to extract the next page link\n",
        "def extract_next_page_url(main_url):\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(main_url, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    next_page_link = soup.find('li', class_='PagedList-skipToNext')\n",
        "    if next_page_link and next_page_link.find('a'):\n",
        "        return next_page_link.find('a')['href']\n",
        "    return None\n",
        "\n",
        "# Main function to process all links and save data to CSV\n",
        "def scrape_and_save_to_csv(main_url, csv_filename):\n",
        "    csv_headers = [\"Title\", \"Author\", \"Summary\", \"Categories\", \"Tags\", \"Rating\", \"Rank\", \"Chapters\"]\n",
        "\n",
        "    with open(csv_filename, mode=\"a\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "        writer = csv.writer(file)\n",
        "        if file.tell() == 0:\n",
        "            writer.writerow(csv_headers)\n",
        "\n",
        "        while main_url:\n",
        "            print(f\"Scraping {main_url}\")\n",
        "            href_links = extract_all_links(main_url)\n",
        "\n",
        "            for link in href_links:\n",
        "                full_url = f\"https://www.webnovelworld.org{link}\"\n",
        "                book_details = extract_book_details(full_url)\n",
        "                writer.writerow([book_details[\"title\"], book_details[\"author\"], book_details[\"summary\"],\n",
        "                                 book_details[\"categories\"], book_details[\"tags\"], book_details[\"rating\"],\n",
        "                                 book_details[\"rank\"], book_details[\"chapters\"]])\n",
        "\n",
        "            next_page_url = extract_next_page_url(main_url)\n",
        "            main_url = next_page_url\n",
        "\n",
        "    print(f\"Data saved to {csv_filename}\")\n",
        "\n",
        "# Start scraping process\n",
        "main_url = 'https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=69'\n",
        "csv_filename = \"/content/novel_data000 (1).csv\"\n",
        "\n",
        "scrape_and_save_to_csv(main_url, csv_filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQDYrxz-4CqQ",
        "outputId": "14b16adc-0ecf-4b02-da42-93ddacd4ca1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=69\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=70\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=71\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=72\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=73\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=74\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=75\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=76\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=77\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=78\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=79\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=80\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=81\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=82\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=83\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=84\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=85\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=86\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=87\n",
            "Data saved to /content/novel_data000 (1).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Function to find missing ranks\n",
        "def find_missing_ranks(csv_filename):\n",
        "    ranks = []\n",
        "\n",
        "    # Read the CSV file and collect the rank column\n",
        "    with open(csv_filename, mode=\"r\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "        reader = csv.reader(file)\n",
        "        header = next(reader)  # Skip the header row\n",
        "        rank_index = header.index(\"Rank\")  # Find the index of the 'Rank' column\n",
        "\n",
        "        for row in reader:\n",
        "            try:\n",
        "                rank = int(row[rank_index])  # Convert rank to integer\n",
        "                ranks.append(rank)\n",
        "            except ValueError:\n",
        "                continue  # Skip rows with invalid rank values\n",
        "\n",
        "    # Find missing ranks in the range 1 to 1305\n",
        "    full_set = set(range(1, 1306))\n",
        "    missing_ranks = full_set - set(ranks)\n",
        "\n",
        "    return sorted(missing_ranks)\n",
        "\n",
        "# Example usage\n",
        "csv_filename = \"/content/novel_data_unique.csv\"\n",
        "missing_ranks = find_missing_ranks(csv_filename)\n",
        "\n",
        "print(f\"Missing ranks: {missing_ranks}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okqLOWAAZxDS",
        "outputId": "c0224e1e-251a-4ee9-eb50-1d74a545b52e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing ranks: [129, 135, 246, 340, 417, 464, 514, 699, 784, 817, 854, 887, 923, 1011, 1114, 1120, 1196, 1293, 1305]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from collections import Counter\n",
        "\n",
        "# Function to find duplicate ranks\n",
        "def find_duplicate_ranks(csv_filename):\n",
        "    ranks = []\n",
        "\n",
        "    # Read the CSV file and collect the rank column\n",
        "    with open(csv_filename, mode=\"r\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "        reader = csv.reader(file)\n",
        "        header = next(reader)  # Skip the header row\n",
        "        rank_index = header.index(\"Rank\")  # Find the index of the 'Rank' column\n",
        "\n",
        "        for row in reader:\n",
        "            try:\n",
        "                rank = int(row[rank_index])  # Convert rank to integer\n",
        "                ranks.append(rank)\n",
        "            except ValueError:\n",
        "                continue  # Skip rows with invalid rank values\n",
        "\n",
        "    # Count occurrences of each rank\n",
        "    rank_counts = Counter(ranks)\n",
        "\n",
        "    # Find ranks that appear more than once\n",
        "    duplicates = {rank: count for rank, count in rank_counts.items() if count > 1}\n",
        "\n",
        "    return duplicates\n",
        "\n",
        "# Example usage\n",
        "csv_filename = \"/content/novel_data_unique.csv\"\n",
        "duplicates = find_duplicate_ranks(csv_filename)\n",
        "\n",
        "if duplicates:\n",
        "    print(f\"Duplicate ranks found: {duplicates}\")\n",
        "else:\n",
        "    print(\"No duplicate ranks found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CESMTgrMlAe4",
        "outputId": "c6fee8a4-3d24-457b-88a2-238733f492cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No duplicate ranks found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Function to remove duplicate rows with the same rank\n",
        "def remove_duplicate_ranks(csv_filename, output_filename):\n",
        "    rows = []\n",
        "\n",
        "    # Read the CSV file and collect all rows\n",
        "    with open(csv_filename, mode=\"r\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "        reader = csv.reader(file)\n",
        "        header = next(reader)  # Get the header row\n",
        "        rank_index = header.index(\"Rank\")  # Find the index of the 'Rank' column\n",
        "        rows = list(reader)\n",
        "\n",
        "    seen_ranks = set()\n",
        "    unique_rows = [header]  # Include the header row in the output\n",
        "\n",
        "    # Keep only the first occurrence of each rank\n",
        "    for row in rows:\n",
        "        rank = row[rank_index]\n",
        "        if rank not in seen_ranks:\n",
        "            unique_rows.append(row)\n",
        "            seen_ranks.add(rank)\n",
        "\n",
        "    # Write the unique rows to a new CSV file\n",
        "    with open(output_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerows(unique_rows)\n",
        "\n",
        "    print(f\"Duplicates removed and saved to {output_filename}\")\n",
        "\n",
        "# Example usage\n",
        "csv_filename = \"/content/novel_data000 (1).csv\"\n",
        "output_filename = \"/content/novel_data_unique.csv\"\n",
        "remove_duplicate_ranks(csv_filename, output_filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYtXxyyilFuU",
        "outputId": "d4e4bf1c-53ac-4985-ba26-cadaf0cefc2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplicates removed and saved to /content/novel_data_unique.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "missing_ranks.remove(1305)"
      ],
      "metadata": {
        "id": "xMHejbIfmokE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_ranks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vNO3gLJp-Nr",
        "outputId": "44ec9bed-5f73-431b-ad2e-5403066e2326"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[129,\n",
              " 135,\n",
              " 246,\n",
              " 340,\n",
              " 417,\n",
              " 464,\n",
              " 514,\n",
              " 699,\n",
              " 784,\n",
              " 817,\n",
              " 854,\n",
              " 887,\n",
              " 923,\n",
              " 1011,\n",
              " 1114,\n",
              " 1120,\n",
              " 1196,\n",
              " 1293]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "\n",
        "# Function to extract book details from the novel page\n",
        "def extract_book_details(book_url):\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(book_url, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extract summary\n",
        "    summary_text = \"N/A\"\n",
        "    summary_container = soup.find('div', class_='summary')\n",
        "    if summary_container:\n",
        "        content_div = summary_container.find('div', class_='content expand-wrapper')\n",
        "        if content_div:\n",
        "            summary_text = content_div.get_text(strip=True).replace(\"Show More\", \"\").strip()\n",
        "\n",
        "    # Extract categories\n",
        "    categories = []\n",
        "    categories_container = soup.find('div', class_='categories')\n",
        "    if categories_container:\n",
        "        category_links = categories_container.find_all('a', class_='property-item')\n",
        "        categories = [cat.get_text(strip=True) for cat in category_links]\n",
        "\n",
        "    # Extract tags\n",
        "    tags = []\n",
        "    tags_container = soup.find('div', class_='tags')\n",
        "    if tags_container:\n",
        "        tags_div = tags_container.find('div', class_='expand-wrapper')\n",
        "        if tags_div:\n",
        "            tags = [tag.get_text(strip=True).replace(\"Show More\", \"\") for tag in tags_div.find_all('a')]\n",
        "\n",
        "    categories_text = \", \".join(categories) if categories else \"N/A\"\n",
        "    tags_text = \", \".join(tags) if tags else \"N/A\"\n",
        "\n",
        "    # Extract author name\n",
        "    author_text = \"N/A\"\n",
        "    author_container = soup.find('div', class_='author')\n",
        "    if author_container:\n",
        "        author_span = author_container.find('span', itemprop='author')\n",
        "        if author_span:\n",
        "            author_text = author_span.get_text(strip=True)\n",
        "\n",
        "    # Extract rating\n",
        "    rating_text = \"N/A\"\n",
        "    rating_container = soup.find('div', class_='rating-star')\n",
        "    if rating_container:\n",
        "        rating_strong = rating_container.find('strong')\n",
        "        if rating_strong:\n",
        "            rating_text = rating_strong.get_text(strip=True)\n",
        "\n",
        "    # Extract rank\n",
        "    rank_text = \"N/A\"\n",
        "    rank_container = soup.find('div', class_='rank')\n",
        "    if rank_container:\n",
        "        rank_strong = rank_container.find('strong')\n",
        "        if rank_strong:\n",
        "            rank_text = rank_strong.get_text(strip=True).replace(\"RANK\", \"\").strip()\n",
        "\n",
        "    # Extract title\n",
        "    title_text = soup.find(\"h1\")\n",
        "    title = title_text.get_text(strip=True) if title_text else \"N/A\"\n",
        "\n",
        "    # Extract chapter count\n",
        "    chapter_count = \"N/A\"\n",
        "\n",
        "    # Search for span containing chapter information\n",
        "    chapter_container = soup.find('div', class_='header-stats')\n",
        "    if chapter_container:\n",
        "        strong_tag = chapter_container.find('strong')\n",
        "        if strong_tag:\n",
        "            chapter_count = strong_tag.get_text(strip=True)\n",
        "\n",
        "    return {\n",
        "        \"title\": title,\n",
        "        \"author\": author_text,\n",
        "        \"summary\": summary_text,\n",
        "        \"categories\": categories_text,\n",
        "        \"tags\": tags_text,\n",
        "        \"rating\": rating_text,\n",
        "        \"rank\": rank_text,\n",
        "        \"chapters\": chapter_count\n",
        "    }\n",
        "\n",
        "# Function to extract rank from the main page novel list\n",
        "def extract_and_check_rank(novel_item):\n",
        "    rank_text = \"N/A\"\n",
        "    rank_badge = novel_item.find('span', class_='badge _bl')\n",
        "    if rank_badge:\n",
        "        rank_str = rank_badge.get_text(strip=True)\n",
        "        if rank_str.startswith(\"R \"):\n",
        "            rank_text = int(rank_str[2:])  # Extract the numeric rank\n",
        "    return rank_text\n",
        "\n",
        "# Function to extract all href links from the main page's novel list and check rank\n",
        "def extract_all_links_and_check_rank(main_url):\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(main_url, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    novel_list_container = soup.find('ul', class_='novel-list col5 m-col4')\n",
        "    href_links = []\n",
        "    if novel_list_container:\n",
        "        novel_list = novel_list_container.find_all('li', class_='novel-item')  # Look for novel items\n",
        "        for novel_item in novel_list:\n",
        "            rank = extract_and_check_rank(novel_item)\n",
        "            if rank in missing_ranks:\n",
        "                # Get the novel URL if the rank is in missing_ranks\n",
        "                link_tag = novel_item.find('a', title=True)\n",
        "                if 'href' in link_tag.attrs:\n",
        "                    href_links.append(link_tag['href'])\n",
        "    return list(set(href_links))\n",
        "\n",
        "# Function to extract the next page link\n",
        "def extract_next_page_url(main_url):\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(main_url, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    next_page_link = soup.find('li', class_='PagedList-skipToNext')\n",
        "    if next_page_link and next_page_link.find('a'):\n",
        "        return next_page_link.find('a')['href']\n",
        "    return None\n",
        "\n",
        "# Main function to process all links and save data to CSV\n",
        "def scrape_and_save_to_csv(main_url, csv_filename):\n",
        "    csv_headers = [\"Title\", \"Author\", \"Summary\", \"Categories\", \"Tags\", \"Rating\", \"Rank\", \"Chapters\"]\n",
        "\n",
        "    with open(csv_filename, mode=\"a\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "        writer = csv.writer(file)\n",
        "        if file.tell() == 0:\n",
        "            writer.writerow(csv_headers)\n",
        "\n",
        "        while main_url:\n",
        "            print(f\"Scraping {main_url}\")\n",
        "            href_links = extract_all_links_and_check_rank(main_url)\n",
        "\n",
        "            for link in href_links:\n",
        "                full_url = f\"https://www.webnovelworld.org{link}\"\n",
        "                book_details = extract_book_details(full_url)\n",
        "                writer.writerow([book_details[\"title\"], book_details[\"author\"], book_details[\"summary\"],\n",
        "                                 book_details[\"categories\"], book_details[\"tags\"], book_details[\"rating\"],\n",
        "                                 book_details[\"rank\"], book_details[\"chapters\"]])\n",
        "\n",
        "            # Extract the next page URL\n",
        "            next_page_url = extract_next_page_url(main_url)\n",
        "            if next_page_url:\n",
        "                if next_page_url.startswith('http'):\n",
        "                    main_url = next_page_url  # Use full URL directly\n",
        "                else:\n",
        "                    main_url = f\"https://www.webnovelworld.org{next_page_url}\"  # Construct full URL if relative\n",
        "            else:\n",
        "                break\n",
        "\n",
        "    print(f\"Data saved to {csv_filename}\")\n",
        "\n",
        "# Start scraping process\n",
        "main_url = 'https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all'\n",
        "csv_filename = \"/content/novel_data_unique.csv\"\n",
        "\n",
        "scrape_and_save_to_csv(main_url, csv_filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xE4sTKmqBB6",
        "outputId": "b0e8b708-8cb0-4baf-8e16-07f4b5639820"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=2\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=3\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=4\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=5\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=6\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=7\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=8\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=9\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=10\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=11\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=12\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=13\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=14\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=15\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=16\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=17\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=18\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=19\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=20\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=21\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=22\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=23\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=24\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=25\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=26\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=27\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=28\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=29\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=30\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=31\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=32\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=33\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=34\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=35\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=36\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=37\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=38\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=39\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=40\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=41\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=42\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=43\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=44\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=45\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=46\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=47\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=48\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=49\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=50\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=51\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=52\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=53\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=54\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=55\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=56\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=57\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=58\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=59\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=60\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=61\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=62\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=63\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=64\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=65\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=66\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=67\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=68\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=69\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=70\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=71\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=72\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=73\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=74\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=75\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=76\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=77\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=78\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=79\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=80\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=81\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=82\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=83\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=84\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=85\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=86\n",
            "Scraping https://www.webnovelworld.org/browse/genre-all-25060123/order-new/status-all?page=87\n",
            "Data saved to /content/novel_data_unique.csv\n"
          ]
        }
      ]
    }
  ]
}